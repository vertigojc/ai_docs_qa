{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install unstructured\n",
    "!pip install tiktoken\n",
    "!pip install chromadb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import MarkdownTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader, UnstructuredHTMLLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "\n",
    "# Store OpenAI key in env var: OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we load the docs and split any that are longer than 1500 characters\n",
    "This could also be done with a different format of doc, by using a different splitter\n",
    "\n",
    "Note: Because ChatGPT can understand markdown, we are loading the docs with the raw\n",
    "`TextLoader` instead of something like the `UnstructuredMarkdownLoader` which strips\n",
    "out the markdown and leaves plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the docs\n",
    "loader = DirectoryLoader('html_pages/', glob=\"**/*\", loader_cls=UnstructuredHTMLLoader)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the docs into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=250)\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, we create a ChromaDB vector store with source metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: .ma_db_storage\n"
     ]
    }
   ],
   "source": [
    "# This will create the Chroma vector database with embeddings for each chunk of text\n",
    "\n",
    "embeddings = OpenAIEmbeddings() # leave embeddings on default text-embedding-ada-002\n",
    "\n",
    "database_persistent_directory = \".html_db_storage\"\n",
    "vector_db = Chroma.from_texts([doc.page_content for doc in docs], embeddings, persist_directory=database_persistent_directory, metadatas=[{\"source\": f\"https://www.multiamory.com/podcast/{Path(*Path(doc.metadata['source']).parts[1:]).as_posix()}\" } for doc in docs])\n",
    "\n",
    "vector_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: .db_storage\n"
     ]
    }
   ],
   "source": [
    "# If changes have been persisted already, we can load from local storage instead of re-creating the database\n",
    "database_persistent_directory = \".ma_db_storage\"\n",
    "\n",
    "vector_db = Chroma(persist_directory=database_persistent_directory, embedding_function=embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Functions\n",
    "\n",
    "Some custom functions to make interacting with the OpenAI API easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "# This function is mostly just so we can use the \"stream\" option to get responses in real-time.\n",
    "\n",
    "def get_response(question: str, history: List[Dict[str, str]], model: str=\"gpt-3.5-turbo\", temperature: float=0.5, stream: bool=True, timeout=None):\n",
    "    if not timeout:\n",
    "        timeout = 2.0 if stream else 60.0\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model= model,\n",
    "        messages= history + [\n",
    "            {'role': 'user', 'content': f\"{question}\"},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        stream=stream,\n",
    "        request_timeout=timeout\n",
    "    )\n",
    "\n",
    "    LINE_LENGTH = 80\n",
    "    this_line_length = 0\n",
    "    full_response = \"\"\n",
    "    if not stream:\n",
    "        response = [{\"choices\": [{\"delta\": {\"content\": f\"{chunk} \"}}]} for chunk in response[\"choices\"][0][\"message\"][\"content\"].split(\" \")]\n",
    "    for chunk in response:\n",
    "        chunk_text = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n",
    "        full_response += chunk_text\n",
    "\n",
    "        if \"\\n\" in chunk_text:\n",
    "            parts = chunk_text.split(\"\\n\")\n",
    "            for part in parts[:-1]:\n",
    "                print(f\"{part}\\n\", end='', flush=True)\n",
    "            this_line_length = 0\n",
    "            chunk_text = parts[-1]\n",
    "        if this_line_length + len(chunk_text) > LINE_LENGTH:\n",
    "            first_char = chunk_text[:1] if chunk_text[:1] in (\" \", \".\", \"!\", \"?\", \",\", \";\", \":\") else \"\"\n",
    "            print(f\"{first_char}\", end='\\n', flush=True)\n",
    "            chunk_text = chunk_text[len(first_char):]\n",
    "            this_line_length = 0\n",
    "        this_line_length += len(chunk_text)\n",
    "        print(f\"{chunk_text}\", end='', flush=True)\n",
    "    return full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_in_text(text):\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\") # This is encoding for the chat models\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Lookup Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns the top k most similar documents to the query\n",
    "\n",
    "doc_lookup = vector_db.similarity_search(query=\"What are the best ways to improve communication in a long-term relationship?\", k=4)\n",
    "for doc in doc_lookup:\n",
    "    print(f\"{doc.metadata['source']}\")\n",
    "    print(f\"{doc.page_content}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the MMR function returns the top k most similar documents to the query, but with a diversity penalty to avoid returning documents that are too similar to each other\n",
    "\n",
    "doc_lookup_mmr = vector_db.max_marginal_relevance_search(query=\"What are the best ways to improve communication in a long-term relationship?\", k=4)\n",
    "for doc in doc_lookup:\n",
    "    print(f\"{doc.metadata['source']}\")\n",
    "    print(f\"{doc.page_content}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_list(documents: List[Document], max_tokens = 1000):\n",
    "    template = \"----------------------------------------\\nDocument {index}: {source}\\n{doc.page_content}\\n\\n\"\n",
    "    total_tokens = 0\n",
    "    result = \"\"\n",
    "    for index, doc in enumerate(documents):\n",
    "        doc_text = template.format(index=index+1, source=doc.metadata[\"source\"].replace(\"multiamory_episode_pages/\", \"\"), doc=doc)\n",
    "        total_tokens += num_tokens_in_text(doc_text)\n",
    "        if total_tokens > max_tokens:\n",
    "            print(f\"Truncating document list to fit within max tokens. Doc list limited to {index+1} documents.\")\n",
    "            break\n",
    "        result += doc_text\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Asking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_from_sources(question):\n",
    "    doc_lookup = vector_db.max_marginal_relevance_search(query=question, k=4)\n",
    "    documents = create_document_list(doc_lookup, max_tokens=1000)\n",
    "    prompt = f\"Here are some relevant excerpts from the Multiamory podcast. Based on those as well as your existing knowledge, answer this question: {question}\\n\\n\" + documents\n",
    "    print(\"Generating answer using excerpts from the following episodes:\\n\" + \"\\n\".join([f\"{doc.metadata['source']}\" for doc in doc_lookup]))\n",
    "    response = get_response(prompt, [{\"role\": \"system\", \"content\": \"You are a compassionate and helpful relationship coach who references the Multiamory Podcast and is knowledgable about relationships. You try to give practical answers and advice to listener questions.\"}], model=\"gpt-3.5-turbo\", temperature=0.5, stream=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncating document list to fit within max tokens. Doc list limited to 3 documents.\n",
      "Generating answer using excerpts from the following episodes:\n",
      "https://www.multiamory.com/podcast/multiamory_episode_pages/255-why-you-make-bad-decisions\n",
      "https://www.multiamory.com/podcast/multiamory_episode_pages/255-why-you-make-bad-decisions\n",
      "https://www.multiamory.com/podcast/multiamory_episode_pages/210-take-the-fight-out-of-your-fights\n",
      "https://www.multiamory.com/podcast/379-relationship-science-for-your-friendships-and-vice-versa\n",
      "Based on the excerpts from the Multiamory podcast, a helpful suggestion for \n",
      "where to go to eat today would be to use the collaborative decision-making \n",
      "process described by Dedeker, Jase, and Emily. One person could come up with \n",
      "five options, and the other person could pick two of those options. Then, the \n",
      "original person could pick one from those two. This process simplifies decision\n",
      "-making and ensures that both people have a say in the final decision. \n",
      "Alternatively, if someone is feeling hungry and their hunger might affect their \n",
      "decision-making, it would be best to grab a snack first or avoid making any \n",
      "important decisions until after they have eaten."
     ]
    }
   ],
   "source": [
    "res = question_from_sources(\"Where should we go to eat today?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
